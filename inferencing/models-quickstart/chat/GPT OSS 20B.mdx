---
title: 'GPT OSS 20B'
description: '> A low-latency reasoning model with adjustable reasoning depth, built for local deployment and agentic workflows'
---

<Frame>
  <img src="/images/gpt-oss-abhiiiman.png" />
</Frame>

## About the Provider 
**OpenAI** is the organization behind GPT OSS 20B. They are a major AI research lab and platform provider known for creating influential generative AI models (like the GPT series). With GPT-OSS, OpenAI is extending its technology into the open-source ecosystem, empowering developers and enterprises to run powerful language models without proprietary restrictions.

## Model Quickstart

This section helps you quickly get started with the `openai/gpt-oss-20b` model on the Qubrid AI inferencing platform.

To use this model, you need:
- A valid **Qubrid API key**
- Access to the Qubrid inference API
- Basic knowledge of making API requests in your preferred language

Once authenticated with your API key, you can send inference requests to the `openai/gpt-oss-20b` model and receive responses based on your input prompts.

Below are example placeholders showing how the model can be accessed using different programming environments.  
You can choose the one that best fits your workflow.

<CodeGroup>
  ```python Python theme={null}
from openai import OpenAI

# Initialize the OpenAI client with Qubrid base URL
client = OpenAI(
    base_url="https://platform.qubrid.com/v1",
    api_key="QUBRID_API_KEY",
)

# Create a streaming chat completion
stream = client.chat.completions.create(
    model="openai/gpt-oss-20b",
    messages=[
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms"
      }
    ],
    max_tokens=4096,
    temperature=0.7,
    top_p=1,
    stream=True
)

# If stream = False comment this out
for chunk in stream:
    if chunk.choices and chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print("\n")

# If stream = True comment this out
print(stream.choices[0].message.content)

```

```js JavaScript theme={null}
import OpenAI from 'openai';

// Initialize the OpenAI client with Qubrid base URL
const client = new OpenAI({
  baseURL: 'https://platform.qubrid.com/v1',
  apiKey: 'QUBRID_API_KEY',
});

// Create a streaming chat completion
const stream = await client.chat.completions.create({
  model: 'openai/gpt-oss-20b',
  messages: [
    {
      "role": "user",
      "content": "Explain quantum computing in simple terms"
    }
  ],
  max_tokens: 4096,
  temperature: 0.7,
  top_p: 1,
  stream: true
});

// If stream = false comment this out
for await (const chunk of stream) {
  if (chunk.choices[0]?.delta?.content) {
    process.stdout.write(chunk.choices[0].delta.content);
  }
}
console.log('\n');

// If stream = true comment this out
console.log(stream.choices[0].message.content);
```

```go Go theme={null}
package main

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"net/http"
)

func main() {
	url := "https://platform.qubrid.com/v1/chat/completions"

	data := map[string]interface{}{
		"model": "openai/gpt-oss-20b",
		"messages": []map[string]string{
			{
				"role":    "user",
				"content": "Explain quantum computing in simple terms",
			},
		},
		"temperature": 0.7,
		"max_tokens":  4096,
		"stream":      true,
		"top_p":       1,
	}

	jsonData, _ := json.Marshal(data)
	req, _ := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	req.Header.Set("Authorization", "Bearer QUBRID_API_KEY")
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{}
	res, _ := client.Do(req)
	defer res.Body.Close()

	scanner := bufio.NewScanner(res.Body)
	for scanner.Scan() {
		line := scanner.Text()
		if line != "" {
			fmt.Println(line)
		}
	}
}
```

```curl cURL theme={null}
curl -X POST "https://platform.qubrid.com/v1/chat/completions" \
  -H "Authorization: Bearer QUBRID_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
  "model": "openai/gpt-oss-20b",
  "messages": [
    {
      "role": "user",
      "content": "Explain quantum computing in simple terms"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 4096,
  "stream": true,
  "top_p": 1
}'
```
</CodeGroup>

## Model Overview

**GPT OSS 20B** is a large language model optimized for low-latency inference, local deployments, and specialized use cases.
It provides strong reasoning capabilities with adjustable reasoning depth, making it suitable for applications that require transparency, control, and efficient execution without large GPU infrastructure.

### Model at a Glance

| Feature | Details |
|---------|---------|
| Model ID | `openai/gpt-oss-20b` |
| Provider | OpenAI |
| Architecture | Compact Mixture-of-Experts (MoE) with SwiGLU activations, Token-choice MoE, Alternating attention mechanism |
| Model Size | 20.9B Params |
| Parameters | 4 |
| Context Length | 131.1k Tokens |
| Training Data | Comprehensive safety evaluation and testing protocols, Global community feedback integration |

### When to use?

You should consider using **GPT OSS 20B** if:

- You need fast, low-latency inference
- You want control over reasoning depth
- Your application benefits from transparent reasoning
- You are building tool-based or agentic workflows
- You want to fine-tune on consumer-grade hardware
This model is not intended as a lightweight chat model, but as a reasoning-focused inference model.

### Reasoning Control
GPT OSS 20B allows you to control how deeply the model reasons before responding.

| Level | What it means |
|-------|--------------|
| Low | Fast responses for simple conversations |
| Medium | Balanced speed and reasoning depth |
| High | Deep, multi-step analysis for complex tasks |

You can set the reasoning level directly in the system prompt:

```plain theme={null}
Reasoning: high
```

### Inference Parameters
| Parameter Name | Type    | Default | Description                                                           |
|----------------|---------|---------|-----------------------------------------------------------------------|
| Streaming      | boolean | true    | Enable streaming responses for real-time output.                     |
| Temperature    | number  | 0.7     | Controls randomness. Higher values mean more creative but less predictable output. |
| Max Tokens     | number  | 4096    | Maximum number of tokens to generate in the response.                |
| Top P          | number  | 1       | Nucleus sampling: considers tokens with top_p probability mass.       |

### Key Features
1. **Low-latency reasoning** – Optimized for fast inference while providing strong reasoning capabilities with adjustable reasoning depth.  
2. **Adjustable reasoning depth** – Allows control over how deeply the model analyzes a problem (low, medium, high) for speed or detailed multi-step reasoning.  
3. **Transparency and debugging** – Provides full chain-of-thought access, making outputs easier to understand and debug.  
4. **Agentic and tool capabilities** – Supports function calling, web browsing, structured outputs, and tool-based workflows for advanced applications.


### Chain-of-Thought Access
The model provides full chain-of-thought visibility, enabling :
- Easier debugging
- Better understanding of how responses are generated
- Increased trust in outputs

### Tool and Agent Capabilities
**GPT OSS 20B** supports agentic workflows and tool usage including:
- Function calling with defined schemas
- Web browsing using built-in browsing tools
- Agentic operations such as browser-based tasks
- Structured outputs


## Summary
**GPT OSS 20B** is a low-latency reasoning model designed for efficient inference and local deployments.
- It provides adjustable reasoning depth to balance speed and analysis.
- The model exposes internal reasoning for transparency and debugging.
- It supports agentic workflows, tool usage, and structured outputs.
- GPT-OSS 20B can be fine-tuned and run on consumer-grade hardware.