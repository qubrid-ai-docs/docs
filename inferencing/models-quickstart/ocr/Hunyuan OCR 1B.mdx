---
title: "Hunyuan OCR 1B"
description: "> Hunyuan OCR (1B) is an end-to-end OCR vision-language model designed for multilingual text extraction and document parsing using a single-inference workflow."
---

## About the Provider
Tencent is a major Chinese technology company and cloud services provider that develops AI models and research technologies through its Hunyuan AI initiative. The company focuses on creating advanced open-source and commercial AI systems—including vision-language, OCR, and foundation models—to support developers, enterprises, and real-world applications across industries.

## Model Quickstart

This section helps you quickly get started with the `tencent/HunyuanOCR` model on the Qubrid AI inferencing platform.

To use this model, you need:

- A valid **Qubrid API key**
- Access to the Qubrid inference API
- Basic knowledge of making API requests in your preferred language

Once authenticated with your API key, you can send inference requests to the `tencent/HunyuanOCR` model and receive responses based on your input prompts.

Below are example placeholders showing how the model can be accessed using different programming environments.  
You can choose the one that best fits your workflow.

<CodeGroup>
  ```python Python theme={null}
from openai import OpenAI

# Initialize the OpenAI client with Qubrid base URL
client = OpenAI(
    base_url="https://platform.qubrid.com/v1",
    api_key="QUBRID_API_KEY",
)

# Create a streaming chat completion
stream = client.chat.completions.create(
    model="tencent/HunyuanOCR",
    messages=[
      {
        "role": "user",
        "content": [
          {
            "type": "image_url",
            "image_url": {
              "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
            }
          }
        ]
      }
    ],
    max_tokens=4096,
    temperature=0,
    stream=True,
    extra_body={
        "language": "en",
        "ocr_mode": "general",
    }
)

# If stream = False comment this out
for chunk in stream:
    if chunk.choices and chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print("\n")

# If stream = True comment this out
print(stream.choices[0].message.content)
```

```js JavaScript theme={null}
import OpenAI from 'openai';

// Initialize the OpenAI client with Qubrid base URL
const client = new OpenAI({
  baseURL: 'https://platform.qubrid.com/v1',
  apiKey: 'QUBRID_API_KEY',
});

// Create a streaming chat completion
const stream = await client.chat.completions.create({
  model: 'tencent/HunyuanOCR',
  messages: [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
          }
        }
      ]
    }
  ],
  max_tokens: 4096,
  temperature: 0,
  stream: true,
  extra_body: {
    language: 'en',
    ocr_mode: 'general',
  }
});

// If stream = false comment this out
for await (const chunk of stream) {
  if (chunk.choices[0]?.delta?.content) {
    process.stdout.write(chunk.choices[0].delta.content);
  }
}
console.log('\n');

// If stream = true comment this out
console.log(stream.choices[0].message.content);
```

```go Go theme={null}
package main
import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"net/http"
)

func main() {
	url := "https://platform.qubrid.com/v1/chat/completions"

	data := map[string]interface{}{
		"model": "tencent/HunyuanOCR",
		"messages": []map[string]interface{}{
			{
				"role": "user",
				"content": []map[string]interface{}{
					{
						"type": "image_url",
						"image_url": map[string]string{
							"url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg",
						},
					},
				},
			},
		},
		"max_tokens":  4096,
		"temperature": 0,
		"stream":      true,
		"language":    "en",
		"ocr_mode":    "general",
	}

	jsonData, _ := json.Marshal(data)
	req, _ := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	req.Header.Set("Authorization", "Bearer QUBRID_API_KEY")
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{}
	res, _ := client.Do(req)
	defer res.Body.Close()

	scanner := bufio.NewScanner(res.Body)
	for scanner.Scan() {
		line := scanner.Text()
		if line != "" {
			fmt.Println(line)
		}
	}
}
```

```curl cURL theme={null}
curl -X POST "https://platform.qubrid.com/v1/chat/completions" \
  -H "Authorization: Bearer QUBRID_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
  "model": "tencent/HunyuanOCR",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
          }
        }
      ]
    }
  ],
  "max_tokens": 4096,
  "temperature": 0,
  "stream": true,
  "language": "en",
  "ocr_mode": "general"
}'
```
</CodeGroup>

## To add multiple images using the OCR model
Hunyuan ocr multiple image support :

<CodeGroup>
  ```python Python theme={null}
from openai import OpenAI
import base64

def image_to_data_url(path):
    with open(path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    return f"data:image/png;base64,{b64}"

# Initialize the OpenAI client with Qubrid base URL
client = OpenAI(
    base_url="https://platform.qubrid.com/v1",
    api_key="QUBRID_API_KEY",
)

# Create a streaming chat completion with multiple images
stream = client.chat.completions.create(
    model="tencent/HunyuanOCR",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": image_to_data_url("/path/to/image1")
                    }
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": image_to_data_url("/path/to/image2")
                    }
                }
            ]
        }
    ],
    max_tokens=4096,
    temperature=0,
    stream=True,
    extra_body={
        "language": "auto",
        "ocr_mode": "general",
    }
)

# Process streaming response
for chunk in stream:
    if chunk.choices and chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

print("\n")
```

```js JavaScript theme={null}
import OpenAI from 'openai';
import fs from 'fs';

function imageToDataURL(path) {
    const buffer = fs.readFileSync(path);
    const base64 = buffer.toString('base64');
    return `data:image/png;base64,${base64}`;
}

// Initialize the OpenAI client with Qubrid base URL
const client = new OpenAI({
    baseURL: 'https://platform.qubrid.com/v1',
    apiKey: 'QUBRID_API_KEY',
});

// Create a streaming chat completion with multiple images
const stream = await client.chat.completions.create({
    model: 'tencent/HunyuanOCR',
    messages: [
        {
            role: 'user',
            content: [
                {
                    type: 'image_url',
                    image_url: {
                        url: imageToDataURL('/path/to/image1')
                    }
                },
                {
                    type: 'image_url',
                    image_url: {
                        url: imageToDataURL('/path/to/image2')
                    }
                }
            ]
        }
    ],
    max_tokens: 4096,
    temperature: 0,
    stream: true,
    extra_body: {
        language: 'auto',
        ocr_mode: 'general',
    }
});

// Process streaming response
for await (const chunk of stream) {
    if (chunk.choices[0]?.delta?.content) {
        process.stdout.write(chunk.choices[0].delta.content);
    }
}
console.log('\n');
```

```go Go theme={null}
package main

import (
	"bufio"
	"bytes"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"net/http"
	"os"
)

func imageToDataURL(path string) (string, error) {
	file, err := os.ReadFile(path)
	if err != nil {
		return "", err
	}
	encoded := base64.StdEncoding.EncodeToString(file)
	return "data:image/png;base64," + encoded, nil
}

func main() {
	url := "https://platform.qubrid.com/v1/chat/completions"

	img1, err := imageToDataURL("/path/to/image1")
	if err != nil {
		panic(err)
	}

	img2, err := imageToDataURL("/path/to/image2")
	if err != nil {
		panic(err)
	}

	data := map[string]interface{}{
		"model": "tencent/HunyuanOCR",
		"messages": []map[string]interface{}{
			{
				"role": "user",
				"content": []map[string]interface{}{
					{
						"type": "image_url",
						"image_url": map[string]string{
							"url": img1,
						},
					},
					{
						"type": "image_url",
						"image_url": map[string]string{
							"url": img2,
						},
					},
				},
			},
		},
		"max_tokens":  4096,
		"temperature": 0,
		"language":    "auto",
		"ocr_mode":    "general",
		"stream":      true,
	}

	jsonData, _ := json.Marshal(data)
	req, _ := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	req.Header.Set("Authorization", "Bearer QUBRID_API_KEY")
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{}
	res, _ := client.Do(req)
	defer res.Body.Close()

	scanner := bufio.NewScanner(res.Body)
	for scanner.Scan() {
		line := scanner.Text()
		if line != "" {
			fmt.Println(line)
		}
	}
}
```

```curl cURL theme={null}
curl -X POST "https://platform.qubrid.com/v1/chat/completions" \
  -H "Authorization: Bearer QUBRID_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
  "model": "tencent/HunyuanOCR",
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
          }
        },
        {
          "type": "image_url",
          "image_url": {
            "url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
          }
        }
      ]
    }
  ],
  "max_tokens": 4096,
  "temperature": 0,
  "stream": true,
  "language": "auto",
  "ocr_mode": "general"
}'
```
</CodeGroup>

- For **PDF inputs**: convert each page of the PDF into images (one image per page) and pass those images sequentially as Base64-encoded inputs using the same multi-image request format shown above.

## Model Overview

**Hunyuan OCR (1B)** is an end-to-end OCR-focused vision-language model built on Hunyuan’s native multimodal architecture.

- It is designed to perform text extraction and document understanding tasks using a single instruction and a single inference step.
- With a lightweight 1B parameter size, the model supports multilingual document parsing and multiple OCR-related tasks while remaining efficient for deployment on inferencing platforms.
- The model is focused purely on OCR workflows and is not intended for general visual question answering.

### Model at a Glance

| Feature        | Details                           |
| -------------- | --------------------------------- |
| Model ID       | `tencent/HunyuanOCR`             |
| Provider       | Tencent                           |
| Parameters     | 1B                                |
| Context Length | 16k tokens                        |
| Model Type     | OCR-focused Vision-Language Model |

### When to use?

You should consider using **Hunyuan OCR (1B)** if:

- You need an OCR-specific model rather than a general-purpose vision model
- Your application involves document parsing, text spotting, or subtitle extraction
- You work with multilingual or mixed-language content
- You prefer an end-to-end OCR model instead of cascading OCR systems
- You require a lightweight model optimized for efficient inference

`Do not use this model for general visual question answering tasks.`

### Key Features

- **Efficient Lightweight Architecture :**
  Built on Hunyuan’s native multimodal architecture, achieving strong OCR performance with only 1B parameters and reduced deployment cost.

- **Comprehensive OCR Coverage :**
  Supports text detection, text recognition, complex document parsing, open-field information extraction, video subtitle extraction, photo translation, and document QA within a single model.

- **End-to-End Inference Workflow :**
  Designed around a single-instruction, single-inference approach, avoiding multi-stage OCR pipelines and cascade errors.

- **Multilingual Document Support :**
  Provides robust support for over 100 languages, including mixed-language documents and varied document types.

### Inference Parameters
| Parameter Name       | Type    | Default | Description                                                                 |
|----------------------|---------|---------|-----------------------------------------------------------------------------|
| Streaming            | boolean | true    | Enable streaming responses for real-time output.                            |
| Language             | select  | en      | Optional language hint to improve OCR accuracy for specific languages.      |
| OCR Mode             | select  | general | Select optimized OCR mode based on the image type.                          |
| Max Output Tokens    | number  | 4096    | Maximum number of tokens for the generated text.                            |
| Temperature          | number  | 0       | Controls randomness. Keep at 0 for accurate text extraction.                |


### Performance Characteristics

#### Strengths

- Lightweight 1B parameter model with strong OCR accuracy
- Native handling of high-resolution images and extreme aspect ratios
- Unified end-to-end architecture without bounding-box error propagation
- Effective recognition of rotated and vertical text
- Strong multilingual and mixed-script support

#### Considerations

- Designed specifically for OCR, not general visual reasoning
- May hallucinate on extremely blurred or low-resolution text
- Throughput depends on visual token density

### Summary

**Hunyuan OCR (1B)** is a lightweight, OCR-focused vision-language model developed by Tencent Hunyuan.
- It performs end-to-end OCR tasks using a single instruction and single inference step.
- The model supports multilingual and mixed-language document parsing across images and videos.
- It is optimized for efficient deployment with a 1B parameter size and fp16 quantization.
- The model is best suited for OCR pipelines rather than general-purpose vision tasks.
